{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbdd32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d53512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872983346207417"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euclidean\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2, 0, 4,5 ])\n",
    "\n",
    "euc_sim = np.sqrt(np.sum((u1 - u3)**2))\n",
    "euc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732b63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.666666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean squared Difference (MSD)\n",
    "# euclidean에서 null 값을 제거한 것들끼리의 유사도\n",
    "\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2, 0, 4, 5])\n",
    "\n",
    "MSD = (2**2+ 3**2+ 1**2)/3\n",
    "MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5e62672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JMSD\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u1_norm = u1/5\n",
    "\n",
    "u3 = np.array([2, 0, 4, 5])\n",
    "u3_norm = u3/5\n",
    "\n",
    "jcd = 3/4\n",
    "MSD = ((u1_norm[0] - u3_norm[0])**2 + (u1_norm[2] - u3_norm[2])**2 + (u1_norm[3] - u3_norm[3])**2)/3\n",
    "\n",
    "JMSD = jcd * (1-MSD)\n",
    "JMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "914b99bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399999999999998"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine\n",
    "u1 = np.array([5, 4, 3])\n",
    "u3 = np.array([3, 5, 4])\n",
    "\n",
    "# cos_sim = np.sum((u1 * u3))/(np.sqrt(np.sum(u1**2)) * np.sqrt(np.sum((u3**2))))\n",
    "cos_sim = np.sum((u1 * u3))/(np.linalg.norm(u1, ord = 2) * np.linalg.norm(u3, ord = 2))\n",
    "\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d93152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17817416127494962"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pearson correlation coefficient (PCC)\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2,    4, 5])\n",
    "\n",
    "u1_mean = np.mean(u1)\n",
    "u3_mean = np.mean(u3)\n",
    "\n",
    "new_u1 = np.array([4, 1, 4])\n",
    "new_u3 = np.array([2, 4, 5])\n",
    "\n",
    "a = new_u1-u1_mean\n",
    "b = new_u3-u3_mean\n",
    "\n",
    "pcc = np.sum(a*b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "735921b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2581988897471611"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constrained Pearson\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2,    4, 5])\n",
    "\n",
    "u1_med = np.median(u1)\n",
    "u3_med = np.median(u3)\n",
    "\n",
    "new_u1 = np.array([4, 1, 4])\n",
    "new_u3 = np.array([2, 4, 5])\n",
    "\n",
    "a = new_u1-u1_med\n",
    "b = new_u3-u3_med\n",
    "\n",
    "pcc = np.sum(a*b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ddfd9",
   "metadata": {},
   "source": [
    "# Predicting rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65a2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762757f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 300)\n",
      "(4, 5, 300)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (numpy.ndarray, dim=tuple), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(A_tilde\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 각 헤드 L_sparse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# L_sparse_h = np.abs(A_tilde).sum(axis=(1,2))\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m L_sparse_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_tilde\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(L_sparse_h\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (numpy.ndarray, dim=tuple), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "# L1 norm\n",
    "\n",
    "H, N, T = 4, 5, 300\n",
    "np.random.seed(0)\n",
    "\n",
    "# attention map\n",
    "A = np.random.rand(H, N, T)\n",
    "print(A.shape)\n",
    "# print(A)\n",
    "\n",
    "# normalization\n",
    "A_sum = A.sum(axis=(1,2), keepdims=True) # 각 헤드의 attention map의 합\n",
    "A_tilde = A / A_sum\n",
    "print(A_tilde.shape)\n",
    "\n",
    "# 각 헤드 L_sparse\n",
    "# L_sparse_h = np.abs(A_tilde).sum(axis=(1,2))\n",
    "L_sparse_h = -torch.sum(A_tilde ** 2, dim = (1,2))\n",
    "print()\n",
    "print(L_sparse_h.shape)\n",
    "print(L_sparse_h)\n",
    "\n",
    "# L_sparse\n",
    "L_sparse = np.sum(L_sparse_h) / H\n",
    "print('L_sparse:', L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7e86381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 300)\n",
      "(4, 5, 300)\n",
      "\n",
      "(4,)\n",
      "[-0.00089222 -0.00089486 -0.00089428 -0.00087901]\n",
      "L_sparse: -0.0008900942127996425\n"
     ]
    }
   ],
   "source": [
    "# L2 norm\n",
    "\n",
    "H, N, T = 4, 5, 300\n",
    "# np.random.seed(0)\n",
    "\n",
    "# attention map\n",
    "A = np.random.rand(H, N, T)\n",
    "print(A.shape)\n",
    "# print(A)\n",
    "\n",
    "# normalization\n",
    "A_sum = A.sum(axis=(1,2), keepdims=True) # 각 헤드의 attention map의 합\n",
    "A_tilde = A / A_sum\n",
    "print(A_tilde.shape)\n",
    "\n",
    "# 각 헤드 L_sparse\n",
    "# L_sparse_h = np.abs(A_tilde).sum(axis=(1,2))\n",
    "L_sparse_h = -(np.power(A_tilde,2).sum(axis=(1,2)))\n",
    "print()\n",
    "print(L_sparse_h.shape)\n",
    "print(L_sparse_h)\n",
    "\n",
    "# L_sparse\n",
    "L_sparse = np.sum(L_sparse_h) / H\n",
    "print('L_sparse:', L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec703a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008941309162695329"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7087450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00089682, -0.00088438, -0.00089418, -0.0008965 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_sparse_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e40737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_sparse: tensor(1., grad_fn=<MeanBackward0>)\n",
      "L_sparse=1.000000000000\n",
      "tensor(5.9592, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadSparsityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the multi-head sparsity regularization:\n",
    "        For each head h, A^{(h)} ∈ R^{N×T} is normalized by its sum,\n",
    "        then L_sparse^{(h)} = ||Â^{(h)}||_1, and\n",
    "        L_sparse = (1/H) * sum_h L_sparse^{(h)}.\n",
    "    Supports input shapes (B,H,N,T) or (B,N,T). Optionally supports a mask over frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-12, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        eps: small constant to avoid division by zero\n",
    "        reduction: 'mean' (default), 'sum', or 'none' over the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        assert reduction in (\"mean\", \"sum\", \"none\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, A: torch.Tensor, frame_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        A : torch.Tensor\n",
    "            Attention maps with shape (B,H,N,T) or (B,N,T).\n",
    "        frame_mask : torch.Tensor, optional\n",
    "            Valid-frame mask of shape (B,T) or (B,1,T). Ones for valid frames, zeros for padded frames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            Scalar loss if reduction != 'none', else (B,) vector.\n",
    "        \"\"\"\n",
    "        if A.ndim == 3:\n",
    "            # (B,N,T) -> add H=1\n",
    "            A = A.unsqueeze(1)  # (B,1,N,T)\n",
    "        elif A.ndim != 4:\n",
    "            raise ValueError(\"A must be (B,H,N,T) or (B,N,T).\")\n",
    "\n",
    "        B, H, N, T = A.shape\n",
    "\n",
    "        # Optionally apply frame mask before normalization (mask invalid frames to zero)\n",
    "        if frame_mask is not None:\n",
    "            if frame_mask.ndim == 2:\n",
    "                frame_mask = frame_mask.unsqueeze(1)  # (B,1,T)\n",
    "            if frame_mask.ndim == 3:\n",
    "                # Broadcast to (B,1,1,T)\n",
    "                frame_mask = frame_mask.unsqueeze(1) if frame_mask.shape[1] != 1 else frame_mask\n",
    "            # reshape to (B,1,1,T) for broadcast over heads and joints\n",
    "            frame_mask = frame_mask.view(B, 1, 1, T)\n",
    "            A = A * frame_mask  # zero-out invalid frames\n",
    "\n",
    "        # Head-wise normalization by total sum over (N,T) so that sum_i,t Â^{(h)}_{i,t} = 1\n",
    "        sums = A.sum(dim=(2, 3), keepdim=True).clamp_min(self.eps)  # (B,H,1,1)\n",
    "        A_tilde = A / sums  # (B,H,N,T)\n",
    "\n",
    "        # L1 per head: ||Â^{(h)}||_1 = sum_{i,t} |Â^{(h)}_{i,t}|\n",
    "        # A_tilde >= 0 assumed (attention weights). If not guaranteed, abs() is safe.\n",
    "        L_sparse_h = A_tilde.abs().sum(dim=(2, 3))  # (B,H)\n",
    "\n",
    "        # L_sparse_h = -torch.sqrt(torch.sum(A_tilde ** 2, dim = (2,3))) + self.eps\n",
    "        # L_sparse_h = -torch.sum(A_tilde ** 2, dim = (2,3))\n",
    "\n",
    "        # # 여기만;;\n",
    "        # # L1 per head: ||Â^{(h)}||_1 = sum_{i,t} |Â^{(h)}_{i,t}|\n",
    "        # # A_tilde >= 0 assumed (attention weights). If not guaranteed, abs() is safe.\n",
    "        # L_sparse_h = A.abs().sum(dim=(2, 3))  # (B,H)\n",
    "\n",
    " \n",
    "\n",
    "        # Average over heads: (1/H) * sum_h L_sparse^{(h)}\n",
    "        L_sparse = L_sparse_h.mean(dim=1)  # (B,)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return L_sparse.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return L_sparse.sum()\n",
    "        else:  # 'none'\n",
    "            return L_sparse  # (B,)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 사용 예시\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    B, H, N, T = 8, 4, 25, 48\n",
    "    A = torch.rand(B, H, N, T, requires_grad=True)  # 예시 attention\n",
    "    y = torch.randint(0, 300, (B,))                 # 예시 라벨\n",
    "    logits = torch.randn(B, 300, requires_grad=True)\n",
    "\n",
    "    # 손실들\n",
    "    ce = nn.CrossEntropyLoss()(logits, y)\n",
    "\n",
    "    sparse_loss_fn = MultiHeadSparsityLoss(eps=1e-12, reduction=\"mean\")\n",
    "    L_sparse = sparse_loss_fn(A)  # \\mathcal{L}_{sparse}\n",
    "\n",
    "    # 가중합 (예: lambda_s = 0.1)\n",
    "    lambda_s = 0.1\n",
    "    loss = ce + lambda_s * L_sparse\n",
    "    loss.backward()\n",
    "\n",
    "    print('L_sparse:', L_sparse)\n",
    "    print(f\"L_sparse={L_sparse.item():.12f}\")\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcf54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
