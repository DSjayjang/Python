{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbdd32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0d53512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.872983346207417"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euclidean\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2, 0, 4,5 ])\n",
    "\n",
    "euc_sim = np.sqrt(np.sum((u1 - u3)**2))\n",
    "euc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732b63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.666666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean squared Difference (MSD)\n",
    "# euclidean에서 null 값을 제거한 것들끼리의 유사도\n",
    "\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2, 0, 4, 5])\n",
    "\n",
    "MSD = (2**2+ 3**2+ 1**2)/3\n",
    "MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5e62672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JMSD\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u1_norm = u1/5\n",
    "\n",
    "u3 = np.array([2, 0, 4, 5])\n",
    "u3_norm = u3/5\n",
    "\n",
    "jcd = 3/4\n",
    "MSD = ((u1_norm[0] - u3_norm[0])**2 + (u1_norm[2] - u3_norm[2])**2 + (u1_norm[3] - u3_norm[3])**2)/3\n",
    "\n",
    "JMSD = jcd * (1-MSD)\n",
    "JMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "914b99bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399999999999998"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine\n",
    "u1 = np.array([5, 4, 3])\n",
    "u3 = np.array([3, 5, 4])\n",
    "\n",
    "# cos_sim = np.sum((u1 * u3))/(np.sqrt(np.sum(u1**2)) * np.sqrt(np.sum((u3**2))))\n",
    "cos_sim = np.sum((u1 * u3))/(np.linalg.norm(u1, ord = 2) * np.linalg.norm(u3, ord = 2))\n",
    "\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d93152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17817416127494962"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pearson correlation coefficient (PCC)\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2,    4, 5])\n",
    "\n",
    "u1_mean = np.mean(u1)\n",
    "u3_mean = np.mean(u3)\n",
    "\n",
    "new_u1 = np.array([4, 1, 4])\n",
    "new_u3 = np.array([2, 4, 5])\n",
    "\n",
    "a = new_u1-u1_mean\n",
    "b = new_u3-u3_mean\n",
    "\n",
    "pcc = np.sum(a*b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "735921b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2581988897471611"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constrained Pearson\n",
    "u1 = np.array([4, 1, 1, 4])\n",
    "u3 = np.array([2,    4, 5])\n",
    "\n",
    "u1_med = np.median(u1)\n",
    "u3_med = np.median(u3)\n",
    "\n",
    "new_u1 = np.array([4, 1, 4])\n",
    "new_u3 = np.array([2, 4, 5])\n",
    "\n",
    "a = new_u1-u1_med\n",
    "b = new_u3-u3_med\n",
    "\n",
    "pcc = np.sum(a*b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "pcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ddfd9",
   "metadata": {},
   "source": [
    "# Predicting rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65a2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b5859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6803, -0.4092, -0.4208,  2.5337, -0.5688],\n",
      "        [-0.5046, -0.2327,  0.1694,  1.2958,  0.0040],\n",
      "        [-0.6674,  0.5148,  0.2724,  0.7052,  0.2956],\n",
      "        [-0.4160, -0.8628, -0.3162,  0.2292, -0.2077],\n",
      "        [ 0.3218, -0.4759,  1.9482, -0.6812,  0.3304]])\n",
      "tensor([[0.8429, 0.3991, 0.3963, 0.9265, 0.3615],\n",
      "        [0.3765, 0.4421, 0.5422, 0.7851, 0.5010],\n",
      "        [0.3391, 0.6259, 0.5677, 0.6693, 0.5734],\n",
      "        [0.3975, 0.2967, 0.4216, 0.5570, 0.4483],\n",
      "        [0.5798, 0.3832, 0.8753, 0.3360, 0.5819]])\n",
      "tensor([[0.2703, 0.0335, 0.0331, 0.6346, 0.0285],\n",
      "        [0.0834, 0.1095, 0.1636, 0.5048, 0.1387],\n",
      "        [0.0747, 0.2436, 0.1912, 0.2947, 0.1957],\n",
      "        [0.1700, 0.1087, 0.1878, 0.3241, 0.2094],\n",
      "        [0.1264, 0.0569, 0.6428, 0.0464, 0.1275]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5,5)\n",
    "print(a)\n",
    "print(torch.sigmoid(a))\n",
    "\n",
    "b=torch.softmax(a, dim=-1)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "743220a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0.0425, 0.0349, 0.0409, 0.0401, 0.0512, 0.0434, 0.0353, 0.0382, 0.0407,\n",
    "         0.0641, 0.0427, 0.0420, 0.0449, 0.0415, 0.0452, 0.0444, 0.0425, 0.0410,\n",
    "         0.0445, 0.0419, 0.0377, 0.0578, 0.0425]\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762757f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_sparse_h: [1. 1. 1. 1.]\n",
      "L_sparse: 1.0\n"
     ]
    }
   ],
   "source": [
    "# L1 norm\n",
    "# 정규화하면 항상 상수\n",
    "np.random.seed(0)\n",
    "\n",
    "# attention map\n",
    "H, N, T = 4, 5, 300\n",
    "A = np.random.rand(H, N, T)\n",
    "\n",
    "# 각 헤드 attention map 합\n",
    "A_sum = A.sum(axis=(1,2), keepdims=True) # (H, 1, 1)\n",
    "\n",
    "# normalization\n",
    "A_tilde = A / A_sum # (H, N, T)\n",
    "\n",
    "# 각 헤드 L_sparse\n",
    "L_sparse_h = np.sum(np.abs(A_tilde), axis=(1,2)) # (H, )\n",
    "print('L_sparse_h:', L_sparse_h)\n",
    "\n",
    "# L_sparse\n",
    "L_sparse = np.sum(L_sparse_h) / H\n",
    "print('L_sparse:', L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ee0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_sparse_h: [751.61065834 761.10900491 730.89206559 734.63562859]\n",
      "L_sparse: 744.5618393542486\n"
     ]
    }
   ],
   "source": [
    "# L1 norm\n",
    "# 정규화안하면? 너무 큼;\n",
    "np.random.seed(0)\n",
    "\n",
    "# attention map\n",
    "H, N, T = 4, 5, 300\n",
    "A = np.random.rand(H, N, T)\n",
    "\n",
    "# # 각 헤드 attention map 합\n",
    "# A_sum = A.sum(axis=(1,2), keepdims=True) # (H, 1, 1)\n",
    "\n",
    "# # normalization\n",
    "# A_tilde = A / A_sum # (H, N, T)\n",
    "\n",
    "# 각 헤드 L_sparse\n",
    "L_sparse_h = np.sum(np.abs(A), axis=(1,2)) # (H, )\n",
    "print('L_sparse_h:', L_sparse_h)\n",
    "\n",
    "# L_sparse\n",
    "L_sparse = np.sum(L_sparse_h) / H\n",
    "print('L_sparse:', L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_sparse_h: [0.02994695 0.02973857 0.02990276 0.02994167]\n",
      "L_sparse: 0.02988248741742505\n"
     ]
    }
   ],
   "source": [
    "# L2 norm\n",
    "# 클수록 sparse? 따라서 loss는 마이너스\n",
    "np.random.seed(0)\n",
    "\n",
    "# attention map\n",
    "H, N, T = 4, 5, 300\n",
    "A = np.random.rand(H, N, T)\n",
    "\n",
    "# 각 헤드 attention map 합\n",
    "A_sum = A.sum(axis=(1,2), keepdims=True) # (H, 1, 1)\n",
    "\n",
    "# normalization\n",
    "A_tilde = A / A_sum # (H, N, T)\n",
    "\n",
    "# 각 헤드 L_sparse\n",
    "L_sparse_h = np.sqrt(np.sum(A_tilde**2, axis=(1,2))) # (H, )\n",
    "print('L_sparse_h:', L_sparse_h)\n",
    "\n",
    "# L_sparse\n",
    "L_sparse = np.sum(L_sparse_h) / H\n",
    "print('L_sparse:', L_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ec703a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00089682, 0.00088438, 0.00089418, 0.0008965 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(A_tilde**2, axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7087450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00089682, -0.00088438, -0.00089418, -0.0008965 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_sparse_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e40737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_sparse: tensor(1., grad_fn=<MeanBackward0>)\n",
      "L_sparse=1.000000000000\n",
      "tensor(5.9592, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiHeadSparsityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the multi-head sparsity regularization:\n",
    "        For each head h, A^{(h)} ∈ R^{N×T} is normalized by its sum,\n",
    "        then L_sparse^{(h)} = ||Â^{(h)}||_1, and\n",
    "        L_sparse = (1/H) * sum_h L_sparse^{(h)}.\n",
    "    Supports input shapes (B,H,N,T) or (B,N,T). Optionally supports a mask over frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-12, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        eps: small constant to avoid division by zero\n",
    "        reduction: 'mean' (default), 'sum', or 'none' over the batch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        assert reduction in (\"mean\", \"sum\", \"none\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, A: torch.Tensor, frame_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        A : torch.Tensor\n",
    "            Attention maps with shape (B,H,N,T) or (B,N,T).\n",
    "        frame_mask : torch.Tensor, optional\n",
    "            Valid-frame mask of shape (B,T) or (B,1,T). Ones for valid frames, zeros for padded frames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            Scalar loss if reduction != 'none', else (B,) vector.\n",
    "        \"\"\"\n",
    "        if A.ndim == 3:\n",
    "            # (B,N,T) -> add H=1\n",
    "            A = A.unsqueeze(1)  # (B,1,N,T)\n",
    "        elif A.ndim != 4:\n",
    "            raise ValueError(\"A must be (B,H,N,T) or (B,N,T).\")\n",
    "\n",
    "        B, H, N, T = A.shape\n",
    "\n",
    "        # Optionally apply frame mask before normalization (mask invalid frames to zero)\n",
    "        if frame_mask is not None:\n",
    "            if frame_mask.ndim == 2:\n",
    "                frame_mask = frame_mask.unsqueeze(1)  # (B,1,T)\n",
    "            if frame_mask.ndim == 3:\n",
    "                # Broadcast to (B,1,1,T)\n",
    "                frame_mask = frame_mask.unsqueeze(1) if frame_mask.shape[1] != 1 else frame_mask\n",
    "            # reshape to (B,1,1,T) for broadcast over heads and joints\n",
    "            frame_mask = frame_mask.view(B, 1, 1, T)\n",
    "            A = A * frame_mask  # zero-out invalid frames\n",
    "\n",
    "        # Head-wise normalization by total sum over (N,T) so that sum_i,t Â^{(h)}_{i,t} = 1\n",
    "        sums = A.sum(dim=(2, 3), keepdim=True).clamp_min(self.eps)  # (B,H,1,1)\n",
    "        A_tilde = A / sums  # (B,H,N,T)\n",
    "\n",
    "        # L1 per head: ||Â^{(h)}||_1 = sum_{i,t} |Â^{(h)}_{i,t}|\n",
    "        # A_tilde >= 0 assumed (attention weights). If not guaranteed, abs() is safe.\n",
    "        L_sparse_h = A_tilde.abs().sum(dim=(2, 3))  # (B,H)\n",
    "\n",
    "        # L_sparse_h = -torch.sqrt(torch.sum(A_tilde ** 2, dim = (2,3))) + self.eps\n",
    "        # L_sparse_h = -torch.sum(A_tilde ** 2, dim = (2,3))\n",
    "\n",
    "        # # 여기만;;\n",
    "        # # L1 per head: ||Â^{(h)}||_1 = sum_{i,t} |Â^{(h)}_{i,t}|\n",
    "        # # A_tilde >= 0 assumed (attention weights). If not guaranteed, abs() is safe.\n",
    "        # L_sparse_h = A.abs().sum(dim=(2, 3))  # (B,H)\n",
    "\n",
    " \n",
    "\n",
    "        # Average over heads: (1/H) * sum_h L_sparse^{(h)}\n",
    "        L_sparse = L_sparse_h.mean(dim=1)  # (B,)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return L_sparse.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return L_sparse.sum()\n",
    "        else:  # 'none'\n",
    "            return L_sparse  # (B,)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 사용 예시\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    B, H, N, T = 8, 4, 25, 48\n",
    "    A = torch.rand(B, H, N, T, requires_grad=True)  # 예시 attention\n",
    "    y = torch.randint(0, 300, (B,))                 # 예시 라벨\n",
    "    logits = torch.randn(B, 300, requires_grad=True)\n",
    "\n",
    "    # 손실들\n",
    "    ce = nn.CrossEntropyLoss()(logits, y)\n",
    "\n",
    "    sparse_loss_fn = MultiHeadSparsityLoss(eps=1e-12, reduction=\"mean\")\n",
    "    L_sparse = sparse_loss_fn(A)  # \\mathcal{L}_{sparse}\n",
    "\n",
    "    # 가중합 (예: lambda_s = 0.1)\n",
    "    lambda_s = 0.1\n",
    "    loss = ce + lambda_s * L_sparse\n",
    "    loss.backward()\n",
    "\n",
    "    print('L_sparse:', L_sparse)\n",
    "    print(f\"L_sparse={L_sparse.item():.12f}\")\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcf54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
